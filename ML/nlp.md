---
title: Natural Language Processing
has_children: false
parent: Machine Learning
nav_order: 5
---

## Textbooks

- [Foundations of Statistical Natural Language Processing](https://www.cs.vassar.edu/~cs366/docs/Manning_Schuetze_StatisticalNLP.pdf), Christopher Manning & Hinrich SchÃ¼tze

   Foundational text on natural language processing. Available as free pdf online (linked)
## Introductory tutorials

- [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)

   Written by an insight alumnus. Begins with the simplest NLP method that could work, and then moves on to more nuanced solutions, such as feature engineering, word vectors, and deep learning.

- [A Review of Bert-based models](https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58)

   A blog post giving a review of BERT-based models and explains transformers at a high level.

- [What you need to know about the new 2019 Transformer Models](https://www.topbots.com/ai-nlp-research-big-language-models/)
  - XLNet: Generalized Autoregressive Pretraining for Language Understanding
  - ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
  - RoBERTa: A Robustly Optimized BERT Pretraining Approach

- [Text Classification Algorithms: A Survey](https://arxiv.org/pdf/1904.08067.pdf)

    This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed

## Tools

- [*Spacy*: Industrial Strength Natural Language Processing](https://spacy.io/)

   A very convenient python package with lots of pre-trained models for NLP

- [ðŸ¤— transformers: State-of-the-art NLP for everyone:](https://huggingface.co/transformers/index.html)

   ðŸ¤— Transformers (formerly known as *pytorch-transformers* and *pytorch-pretrained-bert*) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pre-trained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.
