---
title: Sequence Modeling
has_children: False
parent: Neural Networks
grand_parent: Machine Learning

nav_order: 1
---



- [*Long-Short Term Memory Networks in PyTorch*](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)

- [*NLP From Scratch: Classifying Names with a Character-Level RNN*](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Andrej Karpathy

   Classic blog post describing RNNs and LSTMs.

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), Christopher Olah

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), Jay Alammar

   High level explantation of transformers.

- [Visual explanations of RNN's](https://youtu.be/LHXXI4-IEns)

    Recurrent Neural Networks are an extremely powerful machine learning technique but they may be a little hard to grasp at first. For those just getting into machine learning and deep learning, this is a guide in plain English with helpful visuals to help you grok RNN's.

- [The fall of RNN / LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)

- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)

   Shows ways to augment RNNs: Neural Turing Machines, Attentional Interfaces, Adaptive Computation Time, Neural Programmers

- [*Transformers*: Attention in Disguise](https://www.mihaileric.com/posts/transformers-attention-in-disguise/)

   In this post, we will be describing a class of sequence processing models known as Transformers. Transformers came out on the scene not too long ago and have rocked the natural language processing community because of their pitch: state-of-the-art and efficient sequence processing without recurrent units or convolution.
